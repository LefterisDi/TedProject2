{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600042 Eleftherios Dimitras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600119 Michael Xanthopoulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk                            import word_tokenize\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tokenize                   import TweetTokenizer\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from wordcloud                       import WordCloud\n",
    "from collections                     import Counter\n",
    "from gensim.models                   import Word2Vec\n",
    "from sklearn.manifold                import TSNE\n",
    "from IPython.core.display            import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn                         import svm\n",
    "from sklearn.metrics                 import f1_score\n",
    "from sklearn.neighbors               import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 5\n",
    "menMeans = (20, 35, 30, 35, 27)\n",
    "womenMeans = (25, 32, 34, 20, 25)\n",
    "menStd = (2, 3, 4, 1, 2)\n",
    "womenStd = (3, 5, 2, 3, 3)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, menMeans, width, yerr=menStd)\n",
    "p2 = plt.bar(ind, womenMeans, width,\n",
    "             bottom=menMeans, yerr=womenStd)\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by group and gender')\n",
    "plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))\n",
    "plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Men', 'Women'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEPCAYAAACneLThAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF4pJREFUeJzt3Xtw1NXdx/HPBpIFwiVumwh0RBRMeJp9ApLIUuKCM9aJYbwgDGALodtUW2hhKopigdISg4QiGIFquXdCbSsEqoJAKtaWmKHU7iBNxJAURgeYhjhtWBRhk5B9/uBJxkvHzS789kfOvl8zmclucvZ8z14+OTm7v/NzhEKhkAAARkqwuwAAgHUIeQAwGCEPAAYj5AHAYIQ8ABiMkAcAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAG6x7rDtva2nT+/HklJibK4XDEunsA6JJCoZBaWlqUnJyshITOz89jHvLnz59XXV1drLsFACOkp6erT58+nf79mId8YmKipMuFJiUlxbr7K1JTUyO32213GTEVb2OOt/FKjLmraG5uVl1dXUeGdlbMQ759iSYpKUlOpzPW3V+xrljzlYq3McfbeCXG3JVEuszNG68AYDBCHgAMRsgDgMEIeQAwGCEPAAYj5AHAYIQ8ABiMkAdgm+ZLLbb0m52dbUu/UuzHHPODoQCgXVK3RE15aZbdZcTUtqkvxLQ/ZvIAYDBCHgAMRsgDgMEIeQAwGCEPAAYj5AHAYIQ8ABiMkAcAg4UN+TVr1igjI+MzXz/5yU9UVVWlvLw8eTwelZSUKBQKxaJeAEAEwh7xOmHChI5DgA8fPqxf/vKXmjRpkn74wx8qJydHo0aN0rJly5SVlaXx48dbXjAAoPPCzuRvuOEGjRkzRh6PR/v27dO3vvUttbW1KRAIqKCgQD6fT2lpaaqsrIxFvQCACHR6Tf7111/X+++/r5kzZ+rMmTOSpJSUFEmSy+VSY2OjNRUCAKLW6Q3Kfv/73+vOO+9Uampqx/p7+1nDo1mPr6mpibjNtcDv99tdQszF25jjbbySfWO2czdIO8Xy/u5UyH/yySf6+9//rqVLl0qSUlNTJUlNTU2SpEAgoMzMzIg6drvdcjqdEbWxm9/vj7snZbyNOd7GK8XnmO0Wzf0dDAajmhx3KuSPHj2qlpYWZWVlSZKGDx+u3r17q6ysTPX19WpoaJDX6424cwCAtTq1Jv/hhx9KkgYMGCBJ6tWrl1avXq0TJ05o7dq18vl8ys/Pt65KAEBUOjWTz8/P/0KI5+bmqqKiwpKiAABXB0e8AoDBCHkAMBghDwAGI+QBwGCEPAAYjJAHAIMR8gBgMEIeAAxGyAOAwbpcyDdfarGtb7s2cbJzzAC6tk5vNXytSOqWqCkvzbK7jJjaNvUFu0sA0EV1uZk8AKDzCHkAMBghDwAGI+QBwGCEPAAYjJAHAIMR8gBgMEIeAAzWqZA/fPiwHnjgAd1666167LHH1NLSoqNHj+r+++9Xdna25s+fr4sXL1pdKwAgQmFD/uzZs3rooYc0aNAgPfHEE9qzZ4927NihefPmqWfPnlqwYIFee+01bd26NRb1AgAiEDbk33rrLX388cd69NFH9eCDD2rXrl3yer06fvy4Jk2apEmTJsntduvAgQOxqBcAEIGwe9ecPn1akrR8+XJVVlbqtttu06xZl/eOue666yRJLpdLx48fj6jjmpqaSGuVZN8mYXbz+/1x2bcd4m28kn1j5vVsvbAhHwqFJEkDBw7UM888oyeeeEKrV6/+r78TCbfbLafTGXG7eGXXi8Hv98fVCzHexivF55jtFs39HQwGo5och12u6d+/vyRp8uTJysvLU0ZGhk6ePCnp8nq9JAUCAaWlpUXcOQDAWmFDPjc3Vz169NC6deu0e/du1dfX6+6779agQYNUXl6unTt3qrq6WmPHjo1FvQCACIQN+dTUVD3//POqra3V4sWLNXbsWM2ePVvPPfecLl68qKVLlyo/P18zZsyIRb0AgAh06qQhubm52rNnz2eu+/rXv66XX37ZkqIAAFcHR7wCgMEIeQAwGCEPAAYj5AHAYIQ8ABiMkAcAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGCEPAAYjJAHAIMR8gBgMEIeAAxGyAOAwQh5ADBYp0J+3rx5ysjI6PjatGmTqqqqlJeXJ4/Ho5KSEoVCIatrBQBEqFPneK2urtY999yjiRMnyuFwaPDgwZowYYJycnI0atQoLVu2TFlZWRo/frzV9QIAIhB2Jn/u3Dl98MEHOnDggH7wgx9o+/btOnXqlAKBgAoKCuTz+ZSWlqbKyspY1AsAiEDYmXxDQ4PS09M1YcIE9enTRz/96U+Vnp4uSUpJSZEkuVwuNTY2WlspACBiYUM+PT1dr776asfl3/72t1q7dq0kyeFwSFJU6/E1NTURt5Gk7OzsqNp1dX6/Py77tkO8jVeyb8y8nq0XNuRra2u1f/9+FRYWqlevXmptbVX37t3V2tqqpqYmSVIgEFBmZmZEHbvdbjmdzuiqjkN2vRj8fn9cvRDjbbxSfI7ZbtHc38FgMKrJcdiQv3jxotasWaMPP/xQQ4YMUV1dnRYvXqxVq1aprKxM9fX1amhokNfrjbhzAIC1wob8iBEjtGjRIq1bt067du1SYWGhvv3tb2vw4MEqKiqS3++Xz+dTfn5+LOoFAESgUx+hLCgoUEFBwWeuy83NVUVFhSVFAQCuDo54BQCDEfIAYDBCHgAMRsgDgMEIeQAwGCEPAAYj5AHAYIQ8ABiMkAcAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGCEPAAYjJAHAIMR8gBgMEIeAAwWUcg/9thjysjIkCRVVVUpLy9PHo9HJSUlCoVClhQIAIhep0N+z5492r17tyQpGAxq7ty5GjJkiGbNmqUtW7Zo7969lhUJAIhOp0L+zJkzWrJkiW655RZJ0pEjRxQIBFRQUCCfz6e0tDRVVlZaWigAIHJhQz4UCmnBggUaNWqU8vLyJF0OfUlKSUmRJLlcLjU2NlpYJgAgGt3D/cKLL76od999Vy+//LJ+97vfSZJaW1slSQ6HQ5KiWo+vqamJuI0k/e/w/9W2qS9E1baram5tVvWRatv69/v9Me9zWOb/KLlHr5j3m52dHfM+252/+Ilq333Plr7teIwle+9vO8Xy/g4b8hUVFWpqatK4ceM6rnvuueckSU1NTZKkQCCgzMzMiDp2u91yOp0RtWl3YumkqNp1VTcv3GHbi8Hv99vW95SXZtnSr122TX3Blvvazsc4XkVzfweDwagmx2FD/mc/+5k+/vhjSdL27dtVXl6ulStX6vvf/77KyspUX1+vhoYGeb3eiDsHAFgrbMgPHTq04/v2N1ezs7O1evVqFRUVye/3y+fzKT8/37oqAQBRCRvynzZnzhzNmTNHkpSbm6uKigpLigIAXB0c8doFNF9qsa1v1mqBri2imTzskdQtMS7fhARw5ZjJA4DBCHngGmHXshxLcmZjuQa4RrAsByswkwcAgxHyAGAwQh4ADEbIA4DBeOO1C2hubeYNKgBRIeS7gKTuSXG58yaAK8dyDQAYjJAHAIMR8gBgMEIeAAxGyAOAwQh5ADAYIQ8ABiPkAcBgYUO+ra1NK1asUG5urm6//Xb9+te/liRVVVUpLy9PHo9HJSUlCoVCVtcKAIhQ2JDft2+fNm7cqO9+97saM2aMli1bptraWs2dO1dDhgzRrFmztGXLFu3duzcW9QIAIhA25O+66y5VVFRoxowZGjRokBwOh86dO6dAIKCCggL5fD6lpaWpsrIyFvUCACIQdu+axMREDR48WMXFxdq6daumT5+uM2fOSJJSUlIkSS6XS42NjRF1XFNTE0W5nKoMQNfn9/tj1lenNyibPHmybrzxRhUXF8vtdkuSHA6HJEW1Hu92u+V0OiNuBwBdXTST1WAwGNXkOOxyzfHjx7V3715lZGSooKBAPXv21IYNGyRJTU1NkqRAIKC0tLSIOwcAWCvsTP4f//iHnnzySZ06dUrBYFAXLlzQzJkztWTJEpWVlam+vl4NDQ3yer2xqBcAEIGwIT9hwgTV1dVp48aN6tatm+bMmaP77rtPX/nKV1RUVCS/3y+fz6f8/PxY1AsAiEDYkHc4HJo/f77mz5//metzc3NVUVFhWWEAgCvHmaG6gLbWZs6UBCAqhHwXkNA9Sfc+9ordZcTUrpX3210CYAT2rgEAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGCEPAAYjJAHAIMR8gBgMEIeAAxGyAOAwQh5ADAYIQ8ABiPkAcBghDwAGCxsyIdCIS1fvlyjR4+Wx+NRcXGx2traVFVVpby8PHk8HpWUlCgUCsWiXgBABMKeGWrPnj3avHmz5s+fr5aWFq1atUput1tPP/20cnJyNGrUKC1btkxZWVkaP358LGoGAHRS2Jl8//79NXfuXBUWFmratGmSpFOnTikQCKigoEA+n09paWmqrKy0vFgAQGTCzuSzs7OVnZ0tSXr++eclSd26dZMkpaSkSJJcLpcaGxutqhEAEKVOn8h77dq12rRpk6ZOnaqvfe1rkiSHwyFJUa3H19TURNxGUscfHADoqvx+f8z66lTIr1+/XmvWrNHEiRP185//XIcOHZIkNTU1SZICgYAyMzMj6tjtdsvpdEZYLgB0fdFMVoPBYFST47Ah//bbb2vVqlW6+eabdc899+ivf/2rXC6XevfurbKyMtXX16uhoUFerzfizgEA1gob8i+++KJCoZBOnDihwsJCSVJhYaFWr16toqIi+f1++Xw+5efnW14sACAyYUO+tLRUpaWl//VnFRUVV70gAMDVwxGvAGAwQh4ADEbIA4DBOv05eSCWmlubtW3qC3aXAXR5hDyuSUndk3Ri6SS7y4ipmxfusLsEGIjlGgAwGCEPAAYj5AHAYIQ8ABiMkAcAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGCEPAAYrMttUNbW0hx3Gzm1tTbbXQJigJ03YYUuF/IJiUm697FX7C4jpnatvN/uEhAD7LwJK3R6ueZvf/ubMjIy9Oabb0qSqqqqlJeXJ4/Ho5KSEoVCIcuKBABEJ2zIh0Ih7du3T7Nnz+64LhgMau7cuRoyZIhmzZqlLVu2aO/evZYWCgCIXNiQP3bsmObNmyev19tx3ZEjRxQIBFRQUCCfz6e0tDRVVlZaWigAIHJhQ37AgAHav3+/pkyZ0nHdmTNnJEkpKSmSJJfLpcbGRotKBABEK+wbr/369VO/fv30wQcfdFzXvv7ucDg+czkSNTU1EbeRpOzs7KjaAcC1wu/3x6yvqD5dk5qaKklqamqSJAUCAWVmZkZ0G263W06nM5ruAaBLi2ayGgwGo5ocRxXyw4cPV+/evVVWVqb6+no1NDR8Zs0eAHBtiOqI1169emn16tU6ceKE1q5dK5/Pp/z8/KtdGwDgCnV6Ju/xeHTs2LGOy7m5uaqoqLCkKADA1dHljngFTNXWGn9bdsB6hDxwjUjozpYduPrYhRIADEbIA4DBCHkAMBghDwAGI+QBwGCEPAAYjJAHAIPxOXkAtonH89o2tzYrqXtSzPoj5AHYhvPaWo/lGgAwGCEPAAYj5AHAYIQ8ABiMkAcAgxHyAGAwPkIJwDZtLfF3opS2lmYlJPI5eQBxICGRE6VY7YqWa1555RWNGzdOY8aM0caNG69WTQCAqyTqmXxDQ4MWLlyoBx54QNddd51WrFih2267TcOHD7+a9SFOxeW/8a3NdpcAA0Ud8gcPHlRLS4sefvhhXX/99dqwYYMqKyvDhnwoFJIkNTdH/4ROSe4WdduuKBgMxt2YW9pCemjxbrvLiKmNC++Ku8c5Hp/bwWAwqnbtmdmeoZ3lCEXa4v/96le/0rPPPqu3335bffv2VU5OjsaPH6+ioqIvbffRRx+prq4umi4BIO6lp6erT58+nf79qGfy7X8bHA7HZy6Hk5ycrPT0dCUmJna0BQB8uVAopJaWFiUnJ0fULuqQT01NlSSdPXtWTqdTFy5cUFpaWth2CQkJEf0VAgBc1qNHj4jbRB3yo0ePVrdu3bR+/Xq5XC5dunRJY8eOjfbmAAAWiHpNXrr8EcrS0lIFg0EVFhbqoYceupq1AQCu0BWFPADg2sbeNQBgMEIeAAxGyAOAwQh5ADAYId8JJ0+e1LBhwzR69Ogr2o6hKzh06JAyMjI6vtxut+677z4dPnzY7tIsde7cOS1YsEAej0fZ2dkqLCxUbW2t3WVZ5vOP88iRIzV79mz9+9//trs0y3x+zO1fy5cvt7s0S7HVcCeUl5crFAqpqalJ+/fv1/jx4+0uyXKPPPKIhg8frvPnz2vp0qUqKirSH/7wB7vLskQoFNLDDz+s999/Xz/+8Y/Vp08fPfvss/rOd76j119/XX379rW7RMs88sgjcrvdOn78uNasWaMFCxZo3bp1dpdlqfbndruBAwfaWI31CPkw2tra9Morr+jOO+/Uu+++q+3bt8dFyA8dOlTZ2dk6e/asUlJSojrSrqs4ePCg3nnnHRUXF2vy5MmSpBtvvFHV1dW6dOmSzdVZa9iwYfJ6vfJ6vbpw4YJKS0tVV1en9PR0u0uzTPtzu11iYqKN1ViPkA/jrbfe0r/+9S89+eSTGjx4sDZv3qyTJ0/qhhtusLs0S82ePbvj+4SEBG3YsMHGaqx19OhRSfrM7C4rK0tZWVl2lWQLt9stSfrnP/9pdMh/+rktSWVlZfJ4PDZVYz1CPowdO3aoZ8+eGjFihPr166dNmzapvLxcc+fOtbs0S82fP18jR47U+fPntX79es2ZM0d79+5V//797S7tqmufybW1tdlcybXB9Puh/bndbujQoTZWYz3eeP0SZ8+e1Z/+9CdduHBB48aNk8/nk3Q5+FtbW+0tzmI33XSTRowYodzcXE2bNk2ffPKJ3nnnHbvLskT7rLWmpqbjut27d+t73/ueTp06ZVdZMXf8+HFJlx97k7U/t9u/evfubXdJlmIm/yVeffVVNTc3a/HixR1P/KqqKm3cuFF//vOf9c1vftPmCq1TW1srp9OpYDCorVu3KiEhQcOGDbO7LEuMHj1aI0eO1MqVK3Xp0iU5nU6tWLFCPXv21PXXX293eZZqf5xPnz6tDRs2aMSIEcrMzLS7LEu1j7mdy+Uy9rktsXfNl5owYYLOnTunN954o2Pv+3Pnzsnr9crj8Wj9+vU2V3j1HTp0SDNmzOi4nJCQoJSUFP3oRz/S9OnTbazMWv/5z3/09NNP680335Qk5eTkaOHChRo0aJDNlVnj849zcnKycnNztWjRImP/sH1+zO3uuOMOoz9RRMgDgMFYkwcAgxHyAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGAc8QpjrV+/XuXl5UpOTlZOTo7eeOMNjRo1SmfPntXJkyd1xx13aObMmVqyZIlqa2vlcDjk9Xr16KOPqnv37srIyNDBgwflcrkkqeNyfX29nnnmGQ0cOFAnTpxQjx49VFJSoiFDhtg8YuCLmMnDSJWVldq5c6fKy8u1c+dOnT9/vuNnFy9e1GuvvabHH39cxcXFSklJ0a5du7Rjxw4dO3ZMmzdvDnv7NTU1Kigo0K5duzRx4kQ9/vjjVg4HiBohDyP95S9/0d13362+ffvK4XBo2rRpHT/79F7iBw4c0PTp0+VwOJSUlKQHH3xQBw4cCHv7w4YNU05OjiRp0qRJeu+999TU1HT1BwJcIUIeRurevbs+vWNHt27dOr7v1atXx/dtbW0d+xK1X/5vO4x+/rSPn769L7sOsBshDyONGzdOf/zjH/XRRx9JunwKx//m9ttv129+8xuFQiE1Nzdr27ZtGjNmjKTLuxNWV1dLurz18KfV1tZ2nAP2pZde0q233mr0aQLRdfHGK4z0jW98Q1OmTNHUqVPVo0cP3XLLLerZs+cXfm/RokUqLi7Wvffeq5aWFnm9Xs2cObPjZ0VFRerbt6/GjBmj1NTUjnZf/epXVVpaqtOnT8vlcukXv/hFzMYGRIJdKGGk6upqHT58uGNr2S1btujIkSMqLS294ts+dOiQnnrqqS/M7oFrETN5GOmmm27Shg0btG3bNjkcDg0YMEBPPfWU3WUBMcdMHgAMxhuvAGAwQh4ADEbIA4DBCHkAMBghDwAGI+QBwGD/B+edxCHIxprXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    " \n",
    "# y-axis in bold\n",
    "rc('font', weight='bold')\n",
    " \n",
    "# Values of each group\n",
    "bars1 = [12, 28, 1, 8, 2]\n",
    "bars2 = [28, 7, 16, 4, 2.9]\n",
    "bars3 = [25, 3, 23, 25, 70]\n",
    " \n",
    "# Heights of bars1 + bars2\n",
    "bars = np.add(bars1, bars2).tolist()\n",
    " \n",
    "# The position of the bars on the x-axis\n",
    "r = [0,1,2,3,4]\n",
    " \n",
    "# Names of group and bar width\n",
    "names = ['A','B','C','D','E']\n",
    "barWidth = 1\n",
    " \n",
    "# Create brown bars\n",
    "plt.bar(r, bars1, bottom=None, edgecolor='white', width=barWidth)\n",
    "# Create green bars (middle), on top of the firs ones\n",
    "plt.bar(r, bars2, bottom=bars1, edgecolor='white', width=barWidth)\n",
    "# Create green bars (top)\n",
    "plt.bar(r, bars3, bottom=bars, edgecolor='white', width=barWidth)\n",
    " \n",
    "# Custom X axis\n",
    "plt.xticks(r, names, fontweight='bold')\n",
    "plt.xlabel(\"group\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './crime.csv'\n",
    "df = pd.read_csv(location , sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0, 0, 1, 0, 0]}\n",
      "{'1': [0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "dict2 = {}\n",
    "arr = [0,0,0,0,0]\n",
    "\n",
    "dict1[\"0\"] = [0,0,0,0,0]\n",
    "dict2[\"1\"] = [0,0,0,0,0]\n",
    "\n",
    "dict1[\"0\"][2]+=1\n",
    "\n",
    "print(dict1)\n",
    "print(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100)\n",
      "(1, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "(8, 100)\n",
      "(9, 100)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(1000):\n",
    "    d[i % 10] += 1\n",
    "\n",
    "for item in d.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for val in df.values:\n",
    "#     print(val)\n",
    "from numpy import array\n",
    "\n",
    "dl = df.head(200).values.tolist()\n",
    "# print(df.head(10).values)\n",
    "tokens = []\n",
    "for i,item in enumerate(dl): # remove enumerate()!!!!!!!!!!!!!!!!!!!!!\n",
    "#     print(item[0])\n",
    "    tokens.append(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', ').replace(')\"',')').split('\"'))\n",
    "# print(tokens)\n",
    "# for tok in tokens:\n",
    "#     print(tok)\n",
    "arr = array([tok for tok in tokens])\n",
    "# print(arr)\n",
    "dataframe_dict = {}\n",
    "labels = []\n",
    "for i,item in enumerate(arr):\n",
    "    if i == 0:\n",
    "        for label in item:\n",
    "            dataframe_dict[label] = []\n",
    "            labels.append(label)\n",
    "    else:\n",
    "        for i,col in enumerate(item):\n",
    "            tmp_list = dataframe_dict[labels[i]]\n",
    "            tmp_list.append(col)\n",
    "            dataframe_dict[labels[i]] = tmp_list\n",
    "        \n",
    "#     print(item)\n",
    "dlf = pd.DataFrame(dataframe_dict)\n",
    "ax = sns.countplot(x=\"STREET\", data=dlf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(29) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "from collections import defaultdict\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "crimes    = defaultdict(lambda: 0)\n",
    "years     = defaultdict(lambda: 0)\n",
    "months    = defaultdict(lambda: 0)\n",
    "days      = defaultdict(lambda: 0)\n",
    "districts = defaultdict(lambda: 0)\n",
    "\n",
    "bars_yr   = defaultdict(lambda: 0)\n",
    "bars_mn   = defaultdict(lambda: 0)\n",
    "bars_da   = defaultdict(lambda: 0)\n",
    "bars_di   = defaultdict(lambda: 0)\n",
    "\n",
    "tokens = []\n",
    "for i,item in enumerate(dl): # remove enumerate()!!!!!!!!!!!!!!!!!!!!!\n",
    "#     if i in (15,19):\n",
    "#         print(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', '))\n",
    "#         print(item)\n",
    "    tokens.append(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', ').split('\"'))\n",
    "# print('\\n')\n",
    "cr_cnt=0\n",
    "yr_cnt=0\n",
    "mn_cnt=0\n",
    "da_cnt=0\n",
    "di_cnt=0\n",
    "for tok in tokens:\n",
    "#     if i in (15,19):\n",
    "#         print(tok)\n",
    "#     print(tok[3],tok[8],tok[9],tok[10])\n",
    "    bars_yr[tok[3]] = None\n",
    "    bars_mn[tok[3]] = None\n",
    "    bars_da[tok[3]] = None\n",
    "    bars_di[tok[3]] = None\n",
    "\n",
    "    if tok[3] not in crimes.keys():\n",
    "        crimes[tok[3]] = cr_cnt\n",
    "        cr_cnt += 1\n",
    "    \n",
    "    if tok[8] not in years.keys():\n",
    "        years[tok[8]] = yr_cnt\n",
    "        yr_cnt += 1\n",
    "        \n",
    "    if tok[9] not in months.keys():\n",
    "        months[tok[9]] = mn_cnt\n",
    "        mn_cnt += 1\n",
    "        \n",
    "    if tok[10] not in days.keys():\n",
    "        days[tok[10]] = da_cnt\n",
    "        da_cnt += 1\n",
    "    \n",
    "#     if tok[13] not in districts.keys():\n",
    "    if tok[13] == '':\n",
    "        if 'N/A' not in districts.keys():\n",
    "            districts['N/A'] = di_cnt\n",
    "            di_cnt += 1\n",
    "    elif tok[13] not in districts.keys():\n",
    "        districts[tok[13]] = di_cnt\n",
    "        di_cnt += 1\n",
    "        \n",
    "        \n",
    "for item in crimes:\n",
    "    bars_yr[item] = [0 for i in years.keys()]\n",
    "    bars_mn[item] = [0 for i in months.keys()]\n",
    "    bars_da[item] = [0 for i in days.keys()]\n",
    "    bars_di[item] = [0 for i in districts.keys()]\n",
    "    \n",
    "# init_bars_cr = [0 for i in crimes.keys()]\n",
    "# init_bars_yr = [0 for i in years .keys()]\n",
    "# init_bars_mn = [0 for i in months.keys()]\n",
    "# init_bars_da = [0 for i in days  .keys()]\n",
    "\n",
    "# The position of the bars on the x-axis\n",
    "x_axis_cr = [i for i,_ in enumerate(crimes   .keys())]\n",
    "x_axis_yr = [i for i,_ in enumerate(years    .keys())]\n",
    "x_axis_mn = [i for i,_ in enumerate(months   .keys())]\n",
    "x_axis_da = [i for i,_ in enumerate(days     .keys())]\n",
    "x_axis_di = [i for i,_ in enumerate(districts.keys())]\n",
    "\n",
    "# Names of group\n",
    "label_names_cr = [key for key in crimes   .keys()]\n",
    "label_names_yr = [key for key in years    .keys()]\n",
    "label_names_mn = [key for key in months   .keys()]\n",
    "label_names_da = [key for key in days     .keys()]\n",
    "label_names_di = [key for key in districts.keys()]\n",
    "\n",
    "for tok in tokens:\n",
    "#     print(crimes[tok[3]])\n",
    "#     print(tok[3])\n",
    "    bars_yr[tok[3]][years[tok[8]]]      += 1\n",
    "    bars_mn[tok[3]][months[tok[9]]]     += 1\n",
    "    bars_da[tok[3]][days[tok[10]]]      += 1\n",
    "    if tok[13] == '':\n",
    "        bars_di[tok[3]][districts['N/A']]   += 1\n",
    "    else:\n",
    "        bars_di[tok[3]][districts[tok[13]]] += 1\n",
    "\n",
    "bars = [0 for i in years.keys()]\n",
    "barWidth = 1\n",
    "legends_yr = []\n",
    "for key,val in bars_yr.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_yr, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_yr.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "plt.xticks(x_axis_yr, label_names_yr, fontweight='bold')\n",
    "plt.legend((pl[0] for pl in legends_yr), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "bars = [0 for i in months.keys()]\n",
    "legends_mn = []\n",
    "for key,val in bars_mn.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_mn, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_mn.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "plt.xticks(x_axis_mn, label_names_mn, fontweight='bold')\n",
    "plt.legend((pl[0] for pl in legends_mn), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "bars = [0 for i in days.keys()]\n",
    "legends_da = []\n",
    "for key,val in bars_da.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_da, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_da.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "plt.xticks(x_axis_da, label_names_da, fontweight='bold', rotation=45)\n",
    "plt.legend((pl[0] for pl in legends_da), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "bars = [0 for i in districts.keys()]\n",
    "legends_di = []\n",
    "for key,val in bars_di.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_di, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_di.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "plt.xticks(x_axis_di, label_names_di, fontweight='bold', rotation=90)\n",
    "plt.legend((pl[0] for pl in legends_di), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "# plt.bar(r, bars1, edgecolor='white', width=barWidth)\n",
    "# Create green bars (middle), on top of the firs ones\n",
    "# plt.bar(r, bars2, bottom=bars1, edgecolor='white', width=barWidth)\n",
    "# Create green bars (top)\n",
    "# plt.bar(r, bars3, bottom=bars, edgecolor='white', width=barWidth)\n",
    " \n",
    "# Custom X axis\n",
    "# plt.xticks(x_axis_yr, label_names_yr, fontweight='bold')\n",
    "# plt.xlabel(\"group\")\n",
    "# ax = plt.gca()\n",
    "# plt.legend((pl[0] for pl in legends_yr), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "# plt.legend((pl[0] for pl in legend), (crime for crime in crimes))\n",
    "\n",
    " \n",
    "# Show graphic\n",
    "# plt.show()\n",
    "\n",
    "# for item in years.items():\n",
    "#     print(item)\n",
    "# for item in months.items():\n",
    "#     print(item)\n",
    "# for item in days.items():\n",
    "#     print(item)\n",
    "# for item in districts.items():\n",
    "#     print(item)\n",
    "\n",
    "# for cr in crimes:\n",
    "#     print(cr)\n",
    "\n",
    "# # for i in days.items():\n",
    "# #     print(i)\n",
    "# # print(init_bars_cr)\n",
    "# # print(init_bars_yr)\n",
    "# # print(init_bars_mn)\n",
    "# # print(init_bars_da)\n",
    "\n",
    "# # for tok in tokens:\n",
    "# #     if tok[8] not in crimes_per_year.keys():\n",
    "# #         crimes_per_year[tok[8]] = [tok[3]]\n",
    "# #     else:\n",
    "# #         prev_list = crimes_per_year[tok[8]]\n",
    "# #         prev_list.append(tok[3])\n",
    "# #         crimes_per_year[tok[8]] = prev_list\n",
    "        \n",
    "# #     if tok[9] not in crimes_per_month.keys():\n",
    "# #         crimes_per_month[tok[9]] = [tok[3]]\n",
    "# #     else:\n",
    "# #         prev_list = crimes_per_month[tok[9]]\n",
    "# #         prev_list.append(tok[3])\n",
    "# #         crimes_per_month[tok[9]] = prev_list\n",
    "        \n",
    "# #     if tok[10] not in crimes_per_day.keys():\n",
    "# #         crimes_per_day[tok[10]] = [tok[3]]\n",
    "# #     else:\n",
    "# #         prev_list = crimes_per_day[tok[10]]\n",
    "# #         prev_list.append(tok[3])\n",
    "# #         crimes_per_day[tok[10]] = prev_list\n",
    "\n",
    "# # for i in crimes_per_year.items():\n",
    "# #     print(i)\n",
    "\n",
    "# # for i in crimes_per_month.items():\n",
    "# #     print(i)\n",
    "\n",
    "# # for i in crimes_per_day.items():\n",
    "# #     print(i)\n",
    "    \n",
    "# # # explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "# # # fig = plt.figure(figsize=(18,10), dpi=500)\n",
    "# # #2 rows 2 cols\n",
    "# # #first row, first col\n",
    "# # # ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "# # for i in crimes_per_year.items():\n",
    "# #     plt.pie(i[1], labels=crimes_per_year.values(), autopct='%1.1f%%', startangle=140)\n",
    "# # #     plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "# # # first row sec col\n",
    "# # # ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "# # # plt.pie(ps_data, explode=explode, labels=ps_labels, autopct='%1.1f%%', startangle=140)\n",
    "# # # plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "# # # #Second row first column\n",
    "# # # ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "# # # plt.pie(ng_data, explode=explode, labels=ng_labels, autopct='%1.1f%%', startangle=140)\n",
    "# # # plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "# # # #second row second column\n",
    "# # # ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "# # # plt.pie(nt_data, explode=explode, labels=nt_labels, autopct='%1.1f%%', startangle=140)\n",
    "# # # plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[[16.99 1.01 'Female' ... 'Sun' 'Dinner' 2]\n",
      " [10.34 1.66 'Male' ... 'Sun' 'Dinner' 3]\n",
      " [21.01 3.5 'Male' ... 'Sun' 'Dinner' 3]\n",
      " ...\n",
      " [22.67 2.0 'Male' ... 'Sat' 'Dinner' 2]\n",
      " [17.82 1.75 'Male' ... 'Sat' 'Dinner' 2]\n",
      " [18.78 3.0 'Female' ... 'Thur' 'Dinner' 2]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# ax = sns.countplot(x=\"size\", data=tips)\n",
    "print(type(tips))\n",
    "# print(\"TIPS = \",tips)\n",
    "print(tips.values)\n",
    "for tip in tips.values:\n",
    "    print(type(tip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "emotions    = []\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "#     print(\"\\nITEM = \",item)\n",
    "    tweet = item[3]\n",
    "    emotions.append(item[2])\n",
    "#     if item[2] == \"positive\":\n",
    "#         emotions.append(1)\n",
    "#     elif item[2] == \"negative\":\n",
    "#         emotions.append(-1)\n",
    "#     elif item[2] == \"neutral\":\n",
    "#         emotions.append(0)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "#     tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "#     print(\"TEMP 1 = \",temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "#     print(\"TEMP 2 = \",temp)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "#     print(\"TEMP 4 = \",temp)\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp)\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "# print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "# for i,tok in enumerate(tokens):\n",
    "#     print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "# print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "# print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , fz_count.most_common(10))\n",
    "\n",
    "ps_count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , ps_count.most_common(10))\n",
    "\n",
    "ng_count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , ng_count.most_common(10))\n",
    "\n",
    "nt_count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , nt_count.most_common(10))\n",
    "\n",
    "#The data for the pie chart\n",
    "fz_data   = []\n",
    "fz_labels = []\n",
    "for freq in fz_count.most_common(10):\n",
    "    fz_labels.append(freq[0])\n",
    "    fz_data.append(freq[1])\n",
    "\n",
    "ps_data   = []\n",
    "ps_labels = []\n",
    "for freq in ps_count.most_common(10):\n",
    "    ps_labels.append(freq[0])\n",
    "    ps_data.append(freq[1])\n",
    "\n",
    "ng_data   = []\n",
    "ng_labels = []\n",
    "for freq in ng_count.most_common(10):\n",
    "    ng_labels.append(freq[0])\n",
    "    ng_data.append(freq[1])\n",
    "\n",
    "nt_data   = []\n",
    "nt_labels = []\n",
    "for freq in nt_count.most_common(10):\n",
    "    nt_labels.append(freq[0])\n",
    "    nt_data.append(freq[1])\n",
    "\n",
    "# Data to plot\n",
    "# labels = 'Python', 'C++', 'Ruby', 'Java'\n",
    "# sizes = [215, 130, 245, 210]\n",
    "# colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "fig = plt.figure(figsize=(18,10), dpi=500)\n",
    "#2 rows 2 cols\n",
    "#first row, first col\n",
    "ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "plt.pie(fz_data, explode=explode, labels=fz_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "# first row sec col\n",
    "ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "plt.pie(ps_data, explode=explode, labels=ps_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "#Second row first column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "plt.pie(ng_data, explode=explode, labels=ng_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "#second row second column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "plt.pie(nt_data, explode=explode, labels=nt_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# import numpy as np\n",
    "# fig1 = plt.figure(figsize=(30,10), dpi=100)\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "# labels=[f'{x} {np.round(y/sum(fz_data)*100,1)}%' for x,y in fz_count.most_common(10)]\n",
    "# ax1.pie(fz_data, labels=fz_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,1))\n",
    "# labels=[f'{x} {np.round(y/sum(ps_data)*100,1)}%' for x,y in ps_count.most_common(10)]\n",
    "# ax1.pie(ps_data, labels=ps_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,0))\n",
    "# labels=[f'{x} {np.round(y/sum(ng_data)*100,1)}%' for x,y in ng_count.most_common(10)]\n",
    "# ax1.pie(ng_data, labels=ng_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,1))\n",
    "# labels=[f'{x} {np.round(y/sum(nt_data)*100,1)}%' for x,y in nt_count.most_common(10)]\n",
    "# ax1.pie(nt_data, labels=nt_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# Plot\n",
    "# plt.pie(data, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "# plt.axis('equal')\n",
    "plt.show()\n",
    "print(\"\\033[1;33m*MCW = Most Common Words\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"black\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"red\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"green\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"blue\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "#     print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=7000)\n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets  vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bigbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=7000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"bigtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize = (16,16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy = (x[i], y[i]), xytext = (5,2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum  = 1\n",
    "allDicts = []\n",
    "\n",
    "dictLocation = \"./lexica/generic/generic.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "genericDict = {}\n",
    "for line in file:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(float(word))\n",
    "        count += 1\n",
    "    genericDict[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([genericDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictNum += 1\n",
    "\n",
    "# dictLocation = \"./lexica/emotweet/valence_tweet.txt\"\n",
    "# file = open(dictLocation, \"r\")\n",
    "# vl_dic = []\n",
    "# for line in file:\n",
    "#     print(line)\n",
    "#     temp = []\n",
    "#     count = 1\n",
    "#     for word in line.split():\n",
    "#         print(word)\n",
    "#         if count == 1:\n",
    "#             temp.append(word)\n",
    "#         else:\n",
    "#             temp.append(float(word))\n",
    "#         count += 1\n",
    "#     vl_dic.extend([temp])\n",
    "    \n",
    "# allDicts.extend([vl_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/affin/affin.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "af_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "\n",
    "#     af_dic.extend([temp])\n",
    "    af_dic[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([af_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrc/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrc_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrc_dic[temp[0]] = temp[1]\n",
    "#     nrc_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrc_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrctag/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrctag_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrctag_dic[temp[0]] = temp[1]\n",
    "#     nrctag_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrctag_dic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the tweet vectors and adding the dictionary values to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"bigwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"bigwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow_xtrain, emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcBow = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain_bow: bag of words features for train data\n",
    "# ytrain: train data labels\n",
    "svcBow = svcBow.fit(xtrain_bow, ytrain)\n",
    "\n",
    "# probPrediction = svcBow.predict_proba(xvalid_bow) #predict on the validation set\n",
    "prediction_int = svcBow.predict(xvalid_bow)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_bow_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predbow.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(tfidf , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcIdf = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: tfidf features for train data\n",
    "# ytrain: train data labels\n",
    "svcIdf = svcIdf.fit(xtrain, ytrain) \n",
    "\n",
    "# probPrediction = svcIdf.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_tdidf_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetFeatsList , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V = svcW2V.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V while not using dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetNod , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V_Nod = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V_Nod = svcW2V_Nod.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score_nod = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same type of predictions with the test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/test2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)\n",
    "emoLocation = './twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "emoFile = open(emoLocation, \"r\")\n",
    "testEmotions = []\n",
    "for line in emoFile:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 2:\n",
    "            temp.append(word)\n",
    "        count += 1\n",
    "    testEmotions.extend(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "    tweet = item[3]\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    newTokens.append(final)\n",
    "    \n",
    "bow_xtest = bow_vectorizer.transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets  vocabulary_size) \n",
    "print(bow_xtest.shape)\n",
    "\n",
    "outfile = open(\"testbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtest , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "outfile = open(\"testtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the vectors and adding the dictionary parameteres to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"testwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"testwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data BoW :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcBow.predict_proba(bow_xtrain) #predict on the validation set\n",
    "prediction_int = svcBow.predict(bow_xtest)\n",
    "prediction_int = prediction_int.tolist()\n",
    "svm_bow_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcIdf.predict_proba(tfidf) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(tfidf)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_tdidf_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(allTweetFeatsList)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(allTweetNoDict)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score_nod = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the Bow data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(bow_xtrain , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(bow_xtest)\n",
    "knn_bow_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(knn_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the TFIDF data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(tfidf , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(tfidf_new)\n",
    "knn_tdidf_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(knn_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the W2V data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "w2v = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "w2v_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new)\n",
    "knn_w2v_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction by using W2V data without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_NoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v_NoDict , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_new_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new_nod)\n",
    "knn_w2v_score_nod = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays with the success percentages of the algorithms used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"predtestbow.pkl\" , \"rb\")\n",
    "svm_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtesttfidf.pkl\" , \"rb\")\n",
    "svm_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbs.pkl\" , \"rb\")\n",
    "svm_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "svm_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "col_labels = ['Bow', 'Tf_idf', 'Word_Emb', 'Word_Emb\\nwith dictionaries']\n",
    "row_labels = ['SVM', 'KNN']\n",
    "table_vals = [[str(float('%.2f'%(svm_bow_score * 100))) + \"%\", str(float('%.2f'%(svm_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(svm_w2v_score * 100))) + \"%\", str(float('%.2f'%(svm_w2v_score_nod * 100))) + \"%\"], \n",
    "              [str(float('%.2f'%(knn_bow_score * 100))) + \"%\", str(float('%.2f'%(knn_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(knn_w2v_score * 100))) + \"%\", str(float('%.2f'%(knn_w2v_score_nod * 100))) + \"%\"]]\n",
    "\n",
    "# Draw table\n",
    "the_table = plt.table(cellText  = table_vals,\n",
    "                      colWidths = [0.1, 0.1, 0.12, 0.18],\n",
    "                      rowLabels = row_labels,\n",
    "                      colLabels = col_labels,\n",
    "                      loc       = 'center'  ,\n",
    "                      cellLoc   = 'center',\n",
    "                      rowColours=('#FFFF00','#569857'),\n",
    "                      colColours=(\"#321789\",\"#321789\",\"#321789\",\"#321789\"))\n",
    "\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(24)\n",
    "the_table.scale(4, 4)\n",
    "\n",
    "cellDict = the_table.get_celld()\n",
    "for i in range(0,len(col_labels)):\n",
    "    cellDict[(0,i)].set_height(.3)\n",
    "\n",
    "# Removing ticks and spines enables you to get the figure only with table\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "for pos in ['right','top','bottom','left']:\n",
    "    plt.gca().spines[pos].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BOW   TD_IDF WORD_EMB\n",
    "SVM 51%     55%    42%\n",
    "KNN 48%     48%    48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              charts. \n",
    "       .\n",
    "     word embeddings  dictionaries      \n",
    "     '    .\n",
    "\n",
    "Edit:     ,      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
