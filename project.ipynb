{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600042 Eleftherios Dimitras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600119 Michael Xanthopoulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk                            import word_tokenize\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tokenize                   import TweetTokenizer\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from wordcloud                       import WordCloud\n",
    "from collections                     import Counter\n",
    "from gensim.models                   import Word2Vec\n",
    "from sklearn.manifold                import TSNE\n",
    "from IPython.core.display            import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn                         import svm\n",
    "from sklearn.metrics                 import f1_score\n",
    "from sklearn.neighbors               import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 5\n",
    "menMeans = (20, 35, 30, 35, 27)\n",
    "womenMeans = (25, 32, 34, 20, 25)\n",
    "menStd = (2, 3, 4, 1, 2)\n",
    "womenStd = (3, 5, 2, 3, 3)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, menMeans, width, yerr=menStd)\n",
    "p2 = plt.bar(ind, womenMeans, width,\n",
    "             bottom=menMeans, yerr=womenStd)\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by group and gender')\n",
    "plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))\n",
    "plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Men', 'Women'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    " \n",
    "# y-axis in bold\n",
    "rc('font', weight='bold')\n",
    " \n",
    "# Values of each group\n",
    "bars1 = [12, 28, 1, 8, 22]\n",
    "bars2 = [28, 7, 16, 4, 10]\n",
    "bars3 = [25, 3, 23, 25, 17]\n",
    " \n",
    "# Heights of bars1 + bars2\n",
    "bars = np.add(bars1, bars2).tolist()\n",
    " \n",
    "# The position of the bars on the x-axis\n",
    "r = [0,1,2,3,4]\n",
    " \n",
    "# Names of group and bar width\n",
    "names = ['A','B','C','D','E']\n",
    "barWidth = 1\n",
    " \n",
    "# Create brown bars\n",
    "plt.bar(r, bars1, color='#7f6d5f', edgecolor='white', width=barWidth)\n",
    "# Create green bars (middle), on top of the firs ones\n",
    "plt.bar(r, bars2, bottom=bars1, color='#557f2d', edgecolor='white', width=barWidth)\n",
    "# Create green bars (top)\n",
    "plt.bar(r, bars3, bottom=bars, color='#2d7f5e', edgecolor='white', width=barWidth)\n",
    " \n",
    "# Custom X axis\n",
    "plt.xticks(r, names, fontweight='bold')\n",
    "plt.xlabel(\"group\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './crime.csv'\n",
    "df = pd.read_csv(location , sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "dl = ndf.values.tolist()\n",
    "\n",
    "# dl = df.values.tolist()\n",
    "\n",
    "crimes_per_year  = {}\n",
    "crimes_per_month = {}\n",
    "crimes_per_day   = {}\n",
    "crimes           = {}\n",
    "\n",
    "tokens = []\n",
    "for item in dl:\n",
    "    tokens.append(item[0].split(','))\n",
    "    \n",
    "for tok in tokens:\n",
    "    if tok[8] not in crimes_per_year.keys():\n",
    "        crimes_per_year[tok[8]] = [tok[3]]\n",
    "    else:\n",
    "        prev_list = crimes_per_year[tok[8]]\n",
    "        prev_list.append(tok[3])\n",
    "        crimes_per_year[tok[8]] = prev_list\n",
    "        \n",
    "    if tok[9] not in crimes_per_month.keys():\n",
    "        crimes_per_month[tok[9]] = [tok[3]]\n",
    "    else:\n",
    "        prev_list = crimes_per_month[tok[9]]\n",
    "        prev_list.append(tok[3])\n",
    "        crimes_per_month[tok[9]] = prev_list\n",
    "        \n",
    "    if tok[10] not in crimes_per_day.keys():\n",
    "        crimes_per_day[tok[10]] = [tok[3]]\n",
    "    else:\n",
    "        prev_list = crimes_per_day[tok[10]]\n",
    "        prev_list.append(tok[3])\n",
    "        crimes_per_day[tok[10]] = prev_list\n",
    "\n",
    "for i in crimes_per_year.items():\n",
    "    print(i)\n",
    "\n",
    "for i in crimes_per_month.items():\n",
    "    print(i)\n",
    "\n",
    "for i in crimes_per_day.items():\n",
    "    print(i)\n",
    "    \n",
    "# explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "# fig = plt.figure(figsize=(18,10), dpi=500)\n",
    "#2 rows 2 cols\n",
    "#first row, first col\n",
    "# ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "for i in crimes_per_year.items():\n",
    "    plt.pie(i[1], labels=crimes_per_year.values(), autopct='%1.1f%%', startangle=140)\n",
    "#     plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "# first row sec col\n",
    "# ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "# plt.pie(ps_data, explode=explode, labels=ps_labels, autopct='%1.1f%%', startangle=140)\n",
    "# plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "# #Second row first column\n",
    "# ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "# plt.pie(ng_data, explode=explode, labels=ng_labels, autopct='%1.1f%%', startangle=140)\n",
    "# plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "# #second row second column\n",
    "# ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "# plt.pie(nt_data, explode=explode, labels=nt_labels, autopct='%1.1f%%', startangle=140)\n",
    "# plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "emotions    = []\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "#     print(\"\\nITEM = \",item)\n",
    "    tweet = item[3]\n",
    "    emotions.append(item[2])\n",
    "#     if item[2] == \"positive\":\n",
    "#         emotions.append(1)\n",
    "#     elif item[2] == \"negative\":\n",
    "#         emotions.append(-1)\n",
    "#     elif item[2] == \"neutral\":\n",
    "#         emotions.append(0)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "#     tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "#     print(\"TEMP 1 = \",temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "#     print(\"TEMP 2 = \",temp)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "#     print(\"TEMP 4 = \",temp)\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp)\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "# print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "# for i,tok in enumerate(tokens):\n",
    "#     print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "# print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "# print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , fz_count.most_common(10))\n",
    "\n",
    "ps_count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , ps_count.most_common(10))\n",
    "\n",
    "ng_count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , ng_count.most_common(10))\n",
    "\n",
    "nt_count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , nt_count.most_common(10))\n",
    "\n",
    "#The data for the pie chart\n",
    "fz_data   = []\n",
    "fz_labels = []\n",
    "for freq in fz_count.most_common(10):\n",
    "    fz_labels.append(freq[0])\n",
    "    fz_data.append(freq[1])\n",
    "\n",
    "ps_data   = []\n",
    "ps_labels = []\n",
    "for freq in ps_count.most_common(10):\n",
    "    ps_labels.append(freq[0])\n",
    "    ps_data.append(freq[1])\n",
    "\n",
    "ng_data   = []\n",
    "ng_labels = []\n",
    "for freq in ng_count.most_common(10):\n",
    "    ng_labels.append(freq[0])\n",
    "    ng_data.append(freq[1])\n",
    "\n",
    "nt_data   = []\n",
    "nt_labels = []\n",
    "for freq in nt_count.most_common(10):\n",
    "    nt_labels.append(freq[0])\n",
    "    nt_data.append(freq[1])\n",
    "\n",
    "# Data to plot\n",
    "# labels = 'Python', 'C++', 'Ruby', 'Java'\n",
    "# sizes = [215, 130, 245, 210]\n",
    "# colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "fig = plt.figure(figsize=(18,10), dpi=500)\n",
    "#2 rows 2 cols\n",
    "#first row, first col\n",
    "ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "plt.pie(fz_data, explode=explode, labels=fz_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "# first row sec col\n",
    "ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "plt.pie(ps_data, explode=explode, labels=ps_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "#Second row first column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "plt.pie(ng_data, explode=explode, labels=ng_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "#second row second column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "plt.pie(nt_data, explode=explode, labels=nt_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# import numpy as np\n",
    "# fig1 = plt.figure(figsize=(30,10), dpi=100)\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "# labels=[f'{x} {np.round(y/sum(fz_data)*100,1)}%' for x,y in fz_count.most_common(10)]\n",
    "# ax1.pie(fz_data, labels=fz_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,1))\n",
    "# labels=[f'{x} {np.round(y/sum(ps_data)*100,1)}%' for x,y in ps_count.most_common(10)]\n",
    "# ax1.pie(ps_data, labels=ps_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,0))\n",
    "# labels=[f'{x} {np.round(y/sum(ng_data)*100,1)}%' for x,y in ng_count.most_common(10)]\n",
    "# ax1.pie(ng_data, labels=ng_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,1))\n",
    "# labels=[f'{x} {np.round(y/sum(nt_data)*100,1)}%' for x,y in nt_count.most_common(10)]\n",
    "# ax1.pie(nt_data, labels=nt_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# Plot\n",
    "# plt.pie(data, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "# plt.axis('equal')\n",
    "plt.show()\n",
    "print(\"\\033[1;33m*MCW = Most Common Words\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"black\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"red\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"green\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"blue\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "#     print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=7000)\n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets âˆ™ vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bigbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=7000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"bigtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize = (16,16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy = (x[i], y[i]), xytext = (5,2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum  = 1\n",
    "allDicts = []\n",
    "\n",
    "dictLocation = \"./lexica/generic/generic.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "genericDict = {}\n",
    "for line in file:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(float(word))\n",
    "        count += 1\n",
    "    genericDict[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([genericDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictNum += 1\n",
    "\n",
    "# dictLocation = \"./lexica/emotweet/valence_tweet.txt\"\n",
    "# file = open(dictLocation, \"r\")\n",
    "# vl_dic = []\n",
    "# for line in file:\n",
    "#     print(line)\n",
    "#     temp = []\n",
    "#     count = 1\n",
    "#     for word in line.split():\n",
    "#         print(word)\n",
    "#         if count == 1:\n",
    "#             temp.append(word)\n",
    "#         else:\n",
    "#             temp.append(float(word))\n",
    "#         count += 1\n",
    "#     vl_dic.extend([temp])\n",
    "    \n",
    "# allDicts.extend([vl_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/affin/affin.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "af_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "\n",
    "#     af_dic.extend([temp])\n",
    "    af_dic[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([af_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrc/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrc_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrc_dic[temp[0]] = temp[1]\n",
    "#     nrc_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrc_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrctag/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrctag_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrctag_dic[temp[0]] = temp[1]\n",
    "#     nrctag_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrctag_dic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the tweet vectors and adding the dictionary values to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"bigwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"bigwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow_xtrain, emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcBow = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain_bow: bag of words features for train data\n",
    "# ytrain: train data labels\n",
    "svcBow = svcBow.fit(xtrain_bow, ytrain)\n",
    "\n",
    "# probPrediction = svcBow.predict_proba(xvalid_bow) #predict on the validation set\n",
    "prediction_int = svcBow.predict(xvalid_bow)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_bow_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predbow.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(tfidf , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcIdf = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: tfidf features for train data\n",
    "# ytrain: train data labels\n",
    "svcIdf = svcIdf.fit(xtrain, ytrain) \n",
    "\n",
    "# probPrediction = svcIdf.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_tdidf_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetFeatsList , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V = svcW2V.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V while not using dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetNod , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V_Nod = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V_Nod = svcW2V_Nod.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score_nod = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same type of predictions with the test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/test2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)\n",
    "emoLocation = './twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "emoFile = open(emoLocation, \"r\")\n",
    "testEmotions = []\n",
    "for line in emoFile:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 2:\n",
    "            temp.append(word)\n",
    "        count += 1\n",
    "    testEmotions.extend(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "    tweet = item[3]\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    newTokens.append(final)\n",
    "    \n",
    "bow_xtest = bow_vectorizer.transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets âˆ™ vocabulary_size) \n",
    "print(bow_xtest.shape)\n",
    "\n",
    "outfile = open(\"testbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtest , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "outfile = open(\"testtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the vectors and adding the dictionary parameteres to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"testwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"testwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data BoW :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcBow.predict_proba(bow_xtrain) #predict on the validation set\n",
    "prediction_int = svcBow.predict(bow_xtest)\n",
    "prediction_int = prediction_int.tolist()\n",
    "svm_bow_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcIdf.predict_proba(tfidf) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(tfidf)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_tdidf_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(allTweetFeatsList)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(allTweetNoDict)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score_nod = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the Bow data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(bow_xtrain , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(bow_xtest)\n",
    "knn_bow_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(knn_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the TFIDF data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(tfidf , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(tfidf_new)\n",
    "knn_tdidf_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(knn_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the W2V data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "w2v = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "w2v_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new)\n",
    "knn_w2v_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction by using W2V data without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_NoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v_NoDict , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_new_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new_nod)\n",
    "knn_w2v_score_nod = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays with the success percentages of the algorithms used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"predtestbow.pkl\" , \"rb\")\n",
    "svm_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtesttfidf.pkl\" , \"rb\")\n",
    "svm_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbs.pkl\" , \"rb\")\n",
    "svm_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "svm_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "col_labels = ['Bow', 'Tf_idf', 'Word_Emb', 'Word_Emb\\nwith dictionaries']\n",
    "row_labels = ['SVM', 'KNN']\n",
    "table_vals = [[str(float('%.2f'%(svm_bow_score * 100))) + \"%\", str(float('%.2f'%(svm_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(svm_w2v_score * 100))) + \"%\", str(float('%.2f'%(svm_w2v_score_nod * 100))) + \"%\"], \n",
    "              [str(float('%.2f'%(knn_bow_score * 100))) + \"%\", str(float('%.2f'%(knn_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(knn_w2v_score * 100))) + \"%\", str(float('%.2f'%(knn_w2v_score_nod * 100))) + \"%\"]]\n",
    "\n",
    "# Draw table\n",
    "the_table = plt.table(cellText  = table_vals,\n",
    "                      colWidths = [0.1, 0.1, 0.12, 0.18],\n",
    "                      rowLabels = row_labels,\n",
    "                      colLabels = col_labels,\n",
    "                      loc       = 'center'  ,\n",
    "                      cellLoc   = 'center',\n",
    "                      rowColours=('#FFFF00','#569857'),\n",
    "                      colColours=(\"#321789\",\"#321789\",\"#321789\",\"#321789\"))\n",
    "\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(24)\n",
    "the_table.scale(4, 4)\n",
    "\n",
    "cellDict = the_table.get_celld()\n",
    "for i in range(0,len(col_labels)):\n",
    "    cellDict[(0,i)].set_height(.3)\n",
    "\n",
    "# Removing ticks and spines enables you to get the figure only with table\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "for pos in ['right','top','bottom','left']:\n",
    "    plt.gca().spines[pos].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BOW   TD_IDF WORD_EMB\n",
    "SVM 51%     55%    42%\n",
    "KNN 48%     48%    48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Î”ÎµÎ½ Ï€ÏÏŒÎ»Î±Î²Îµ Î½Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏŽÏƒÎµÎ¹ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· ÏŽÏƒÏ„Îµ Î½Î± Ï†Î±Î¯Î½Î¿Î½Ï„Î±Î¹ ÏŒÎ»Î± Ï„Î± Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± charts. \n",
    "ÎŸ Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ Ï€Î¯Î½Î±ÎºÎ±Ï‚ Ï€ÏÎ¿Î­ÎºÏ…ÏˆÎµ ÏƒÎµ Î¼Î¹Î± Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î· ÎµÎºÏ„Î­Î»ÎµÏƒÎ·.\n",
    "ÎˆÏ‡ÎµÎ¹ Ï…Î»Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ ÎºÎ±Î¹ ÎµÎºÎ´Î¿Ï‡Î® Î¼Îµ word embeddings Ï‡Ï‰ÏÎ¯Ï‚ dictionaries Î· Î¿Ï€Î¿Î¯Î± Î´ÎµÎ½ Ï€ÏÏŒÎ»Î±Î²Îµ Î½Î± \n",
    "Î¿Î»ÎºÎ»Î·ÏÏŽÏƒÎµÎ¹ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ® Ï„Î·Ï‚ ÎºÎ±Î¹ Î³Î¹'Î±Ï…Ï„ÏŒ Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î¼Îµ Ï€Î¿ÏƒÎ¿ÏƒÏ„Î¬ Î­Ï„Î¿Î¹Î¼Î±.\n",
    "\n",
    "Edit: Î£Ï‰ÏƒÏ„Î® Î­ÎºÎ´Î¿ÏƒÎ· Ï„Î¿Ï… Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚ , Î· Î¿Ï€Î¿Î¯Î± Ï„ÎµÎ»ÎµÎ¯Ï‰ÏƒÎµ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ® Ï„Î·Ï‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
