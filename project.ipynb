{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600042 Eleftherios Dimitras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdi1600119 Michael Xanthopoulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import pickle\n",
    "# import string\n",
    "# import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "# from nltk                            import word_tokenize\n",
    "# from nltk.corpus                     import stopwords\n",
    "# from nltk.tokenize                   import TweetTokenizer\n",
    "# from nltk.stem.porter                import PorterStemmer\n",
    "# from wordcloud                       import WordCloud\n",
    "# from collections                     import Counter\n",
    "# from gensim.models                   import Word2Vec\n",
    "# from sklearn.manifold                import TSNE\n",
    "# from IPython.core.display            import HTML\n",
    "# from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "# from sklearn.model_selection         import train_test_split\n",
    "# from sklearn                         import svm\n",
    "# from sklearn.metrics                 import f1_score\n",
    "# from sklearn.neighbors               import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 5\n",
    "menMeans = (20, 35, 30, 35, 27)\n",
    "womenMeans = (25, 32, 34, 20, 25)\n",
    "menStd = (2, 3, 4, 1, 2)\n",
    "womenStd = (3, 5, 2, 3, 3)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, menMeans, width, yerr=menStd)\n",
    "p2 = plt.bar(ind, womenMeans, width,\n",
    "             bottom=menMeans, yerr=womenStd)\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by group and gender')\n",
    "plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))\n",
    "plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Men', 'Women'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    " \n",
    "# y-axis in bold\n",
    "rc('font', weight='bold')\n",
    " \n",
    "# Values of each group\n",
    "bars1 = [12, 28, 1, 8, 2]\n",
    "bars2 = [28, 7, 16, 4, 2.9]\n",
    "bars3 = [25, 3, 23, 25, 70]\n",
    " \n",
    "# Heights of bars1 + bars2\n",
    "bars = np.add(bars1, bars2).tolist()\n",
    " \n",
    "# The position of the bars on the x-axis\n",
    "r = [0,1,2,3,4]\n",
    " \n",
    "# Names of group and bar width\n",
    "names = ['A','B','C','D','E']\n",
    "barWidth = 1\n",
    " \n",
    "# Create brown bars\n",
    "plt.bar(r, bars1, bottom=None, edgecolor='white', width=barWidth)\n",
    "# Create green bars (middle), on top of the firs ones\n",
    "plt.bar(r, bars2, bottom=bars1, edgecolor='white', width=barWidth)\n",
    "# Create green bars (top)\n",
    "plt.bar(r, bars3, bottom=bars, edgecolor='white', width=barWidth)\n",
    " \n",
    "# Custom X axis\n",
    "plt.xticks(r, names, fontweight='bold')\n",
    "plt.xlabel(\"group\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./crime.csv\" , sep = \"\\t\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0, 0, 1, 0, 0]}\n",
      "{'1': [0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "dict2 = {}\n",
    "arr = [0,0,0,0,0]\n",
    "\n",
    "dict1[\"0\"] = [0,0,0,0,0]\n",
    "dict2[\"1\"] = [0,0,0,0,0]\n",
    "\n",
    "dict1[\"0\"][2]+=1\n",
    "\n",
    "print(dict1)\n",
    "print(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100)\n",
      "(1, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "(8, 100)\n",
      "(9, 100)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(1000):\n",
    "    d[i % 10] += 1\n",
    "\n",
    "for item in d.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for val in df.values:\n",
    "# #     print(val)\n",
    "# from numpy import array\n",
    "\n",
    "# # for item in df.values:\n",
    "# #     print(item)\n",
    "# dl = df.values.tolist()\n",
    "# # print(df.head(10).values)\n",
    "\n",
    "# tokens = []\n",
    "# for i,item in enumerate(dl): # remove enumerate()!!!!!!!!!!!!!!!!!!!!!\n",
    "# #     print(item[0])\n",
    "#     tokens.append(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', ').split('\"'))\n",
    "# # print(tokens)\n",
    "# # for tok in tokens:\n",
    "# #     print(tok)\n",
    "# arr = array([tok for tok in tokens])\n",
    "# # print(arr)\n",
    "# dataframe_dict = {}\n",
    "# labels = []\n",
    "# for i,item in enumerate(arr):\n",
    "# #     if i > 365:\n",
    "# #         print(item)\n",
    "#     if i == 0:\n",
    "#         for label in item:\n",
    "#             dataframe_dict[label] = []\n",
    "#             labels.append(label)\n",
    "#     else:\n",
    "#         for i,col in enumerate(item):\n",
    "#             if i < 17:\n",
    "#                 tmp_list = dataframe_dict[labels[i]]\n",
    "#                 tmp_list.append(col)\n",
    "#                 dataframe_dict[labels[i]] = tmp_list\n",
    "        \n",
    "# #     print(item)\n",
    "# dlf = pd.DataFrame(dataframe_dict)\n",
    "# # for val in dlf.values:\n",
    "# #     val[1] = int(val[1])\n",
    "# #     if val[5] != '':\n",
    "# #         val[5] = int(val[5])\n",
    "# #     val[8] = int(val[8])\n",
    "# #     val[9] = int(val[9])\n",
    "# #     val[11] = int(val[11])\n",
    "# #     val[14] = float(val[14])\n",
    "# #     val[15] = float(val[15])\n",
    "# #     print(val)\n",
    "# # print(dlf.values)\n",
    "# # dlf.head()\n",
    "# # dlf.index\n",
    "# # gp = dlf.groupby('YEAR').count()\n",
    "# # gp\n",
    "# # ax = sns.barplot(x=\"YEAR\", y=\"OFFENSE_DESCRIPTION\", data=dlf)\n",
    "# # ax = sns.countplot(x=\"STREET\", data=dlf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   YEAR  OFFENSE_DESCRIPTION\n",
      "0  2015                53392\n",
      "1  2016                99134\n",
      "2  2017               100938\n",
      "3  2018                74356\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpdata = df.groupby('YEAR').count()\n",
    "lst = []\n",
    "for data in gpdata.iterrows():\n",
    "    \n",
    "    tmp = []\n",
    "    tmp.append(data[0]);\n",
    "    tmp.append(data[1][0]);\n",
    "    \n",
    "    lst.append(tmp);\n",
    "    \n",
    "    \n",
    "ldf = pd.DataFrame(lst , columns=['YEAR' , 'OFFENSE_DESCRIPTION'])\n",
    "\n",
    "print(ldf)\n",
    "\n",
    "ax = sns.barplot(x='YEAR' , y='OFFENSE_DESCRIPTION' , data=ldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2ebde78dc35c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#         print(tok)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#     print(tok[3],tok[8],tok[9],tok[10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mbars_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mbars_mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mbars_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# ndf = df.head(29) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "from collections import defaultdict\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "crimes    = defaultdict(lambda: 0)\n",
    "years     = defaultdict(lambda: 0)\n",
    "months    = defaultdict(lambda: 0)\n",
    "days      = defaultdict(lambda: 0)\n",
    "# districts = defaultdict(lambda: 0)\n",
    "\n",
    "bars_yr   = defaultdict(lambda: 0)\n",
    "bars_mn   = defaultdict(lambda: 0)\n",
    "bars_da   = defaultdict(lambda: 0)\n",
    "# bars_di   = defaultdict(lambda: 0)\n",
    "\n",
    "tokens = []\n",
    "for i,item in enumerate(dl): # remove enumerate()!!!!!!!!!!!!!!!!!!!!!\n",
    "#     if i in (15,19):\n",
    "#         print(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', '))\n",
    "#         print(item)\n",
    "    tokens.append(item[0].replace(', ,',',,').replace(', ','~~').replace(',\"','\"').replace('\",','\"').replace(',','\"').replace('~~',', ').split('\"'))\n",
    "# print('\\n')\n",
    "cr_cnt=0\n",
    "yr_cnt=0\n",
    "mn_cnt=0\n",
    "da_cnt=0\n",
    "di_cnt=0\n",
    "for tok in tokens:\n",
    "#     if i in (15,19):\n",
    "#         print(tok)\n",
    "#     print(tok[3],tok[8],tok[9],tok[10])\n",
    "    bars_yr[tok[3]] = None\n",
    "    bars_mn[tok[3]] = None\n",
    "    bars_da[tok[3]] = None\n",
    "    bars_di[tok[3]] = None\n",
    "\n",
    "    if tok[3] not in crimes.keys():\n",
    "        crimes[tok[3]] = cr_cnt\n",
    "        cr_cnt += 1\n",
    "    \n",
    "    if tok[8] not in years.keys():\n",
    "        years[tok[8]] = yr_cnt\n",
    "        yr_cnt += 1\n",
    "        \n",
    "    if tok[9] not in months.keys():\n",
    "        months[tok[9]] = mn_cnt\n",
    "        mn_cnt += 1\n",
    "        \n",
    "    if tok[10] not in days.keys():\n",
    "        days[tok[10]] = da_cnt\n",
    "        da_cnt += 1\n",
    "    \n",
    "#     if tok[13] not in districts.keys():\n",
    "    if tok[13] == '':\n",
    "        if 'N/A' not in districts.keys():\n",
    "            districts['N/A'] = di_cnt\n",
    "            di_cnt += 1\n",
    "    elif tok[13] not in districts.keys():\n",
    "        districts[tok[13]] = di_cnt\n",
    "        di_cnt += 1\n",
    "        \n",
    "        \n",
    "for item in crimes:\n",
    "    bars_yr[item] = [0 for i in years.keys()]\n",
    "    bars_mn[item] = [0 for i in months.keys()]\n",
    "    bars_da[item] = [0 for i in days.keys()]\n",
    "#     bars_di[item] = [0 for i in districts.keys()]\n",
    "    \n",
    "# init_bars_cr = [0 for i in crimes.keys()]\n",
    "# init_bars_yr = [0 for i in years .keys()]\n",
    "# init_bars_mn = [0 for i in months.keys()]\n",
    "# init_bars_da = [0 for i in days  .keys()]\n",
    "\n",
    "# The position of the bars on the x-axis\n",
    "x_axis_cr = [i for i,_ in enumerate(crimes   .keys())]\n",
    "x_axis_yr = [i for i,_ in enumerate(years    .keys())]\n",
    "x_axis_mn = [i for i,_ in enumerate(months   .keys())]\n",
    "x_axis_da = [i for i,_ in enumerate(days     .keys())]\n",
    "# x_axis_di = [i for i,_ in enumerate(districts.keys())]\n",
    "\n",
    "# Names of group\n",
    "label_names_cr = [key for key in crimes   .keys()]\n",
    "label_names_yr = [key for key in years    .keys()]\n",
    "label_names_mn = [key for key in months   .keys()]\n",
    "label_names_da = [key for key in days     .keys()]\n",
    "# label_names_di = [key for key in districts.keys()]\n",
    "\n",
    "for tok in tokens:\n",
    "#     print(crimes[tok[3]])\n",
    "#     print(tok[3])\n",
    "    bars_yr[tok[3]][years[tok[8]]]      += 1\n",
    "    bars_mn[tok[3]][months[tok[9]]]     += 1\n",
    "    bars_da[tok[3]][days[tok[10]]]      += 1\n",
    "#     if tok[13] == '':\n",
    "#         bars_di[tok[3]][districts['N/A']]   += 1\n",
    "#     else:\n",
    "#         bars_di[tok[3]][districts[tok[13]]] += 1\n",
    "\n",
    "bars = [0 for i in years.keys()]\n",
    "barWidth = 1\n",
    "legends_yr = []\n",
    "for key,val in bars_yr.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_yr, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_yr.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "plt.xticks(x_axis_yr, label_names_yr, fontweight='bold')\n",
    "plt.legend((pl[0] for pl in legends_yr), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "bars = [0 for i in months.keys()]\n",
    "legends_mn = []\n",
    "for key,val in bars_mn.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_mn, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_mn.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "plt.xticks(x_axis_mn, label_names_mn, fontweight='bold')\n",
    "plt.legend((pl[0] for pl in legends_mn), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "bars = [0 for i in days.keys()]\n",
    "legends_da = []\n",
    "for key,val in bars_da.items():\n",
    "#     print(key,val)\n",
    "    pl = plt.bar(x_axis_da, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "    legends_da.append(pl)\n",
    "    bars = np.add(bars,val).tolist()\n",
    "\n",
    "plt.xticks(x_axis_da, label_names_da, fontweight='bold', rotation=45)\n",
    "plt.legend((pl[0] for pl in legends_da), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# bars = [0 for i in districts.keys()]\n",
    "# legends_di = []\n",
    "# for key,val in bars_di.items():\n",
    "# #     print(key,val)\n",
    "#     pl = plt.bar(x_axis_di, val, bottom=bars, edgecolor='white', width=barWidth)\n",
    "#     legends_di.append(pl)\n",
    "#     bars = np.add(bars,val).tolist()\n",
    "\n",
    "# plt.xticks(x_axis_di, label_names_di, fontweight='bold', rotation=90)\n",
    "# plt.legend((pl[0] for pl in legends_di), (crime for crime in crimes), bbox_to_anchor=(1.1, 1.1), bbox_transform=plt.gca().transAxes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style='whitegrid')\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# ax = sns.countplot(x=\"size\", data=tips)\n",
    "print(type(tips))\n",
    "# print(\"TIPS = \",tips)\n",
    "print(tips.values)\n",
    "for tip in tips.values:\n",
    "    print(type(tip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "emotions    = []\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "#     print(\"\\nITEM = \",item)\n",
    "    tweet = item[3]\n",
    "    emotions.append(item[2])\n",
    "#     if item[2] == \"positive\":\n",
    "#         emotions.append(1)\n",
    "#     elif item[2] == \"negative\":\n",
    "#         emotions.append(-1)\n",
    "#     elif item[2] == \"neutral\":\n",
    "#         emotions.append(0)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "#     tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "#     print(\"TEMP 1 = \",temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "#     print(\"TEMP 2 = \",temp)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "#     print(\"TEMP 4 = \",temp)\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp)\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "# print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "# for i,tok in enumerate(tokens):\n",
    "#     print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "# print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "# print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , fz_count.most_common(10))\n",
    "\n",
    "ps_count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , ps_count.most_common(10))\n",
    "\n",
    "ng_count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , ng_count.most_common(10))\n",
    "\n",
    "nt_count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , nt_count.most_common(10))\n",
    "\n",
    "#The data for the pie chart\n",
    "fz_data   = []\n",
    "fz_labels = []\n",
    "for freq in fz_count.most_common(10):\n",
    "    fz_labels.append(freq[0])\n",
    "    fz_data.append(freq[1])\n",
    "\n",
    "ps_data   = []\n",
    "ps_labels = []\n",
    "for freq in ps_count.most_common(10):\n",
    "    ps_labels.append(freq[0])\n",
    "    ps_data.append(freq[1])\n",
    "\n",
    "ng_data   = []\n",
    "ng_labels = []\n",
    "for freq in ng_count.most_common(10):\n",
    "    ng_labels.append(freq[0])\n",
    "    ng_data.append(freq[1])\n",
    "\n",
    "nt_data   = []\n",
    "nt_labels = []\n",
    "for freq in nt_count.most_common(10):\n",
    "    nt_labels.append(freq[0])\n",
    "    nt_data.append(freq[1])\n",
    "\n",
    "# Data to plot\n",
    "# labels = 'Python', 'C++', 'Ruby', 'Java'\n",
    "# sizes = [215, 130, 245, 210]\n",
    "# colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "fig = plt.figure(figsize=(18,10), dpi=500)\n",
    "#2 rows 2 cols\n",
    "#first row, first col\n",
    "ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "plt.pie(fz_data, explode=explode, labels=fz_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "# first row sec col\n",
    "ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "plt.pie(ps_data, explode=explode, labels=ps_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "#Second row first column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "plt.pie(ng_data, explode=explode, labels=ng_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "#second row second column\n",
    "ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "plt.pie(nt_data, explode=explode, labels=nt_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# import numpy as np\n",
    "# fig1 = plt.figure(figsize=(30,10), dpi=100)\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "# labels=[f'{x} {np.round(y/sum(fz_data)*100,1)}%' for x,y in fz_count.most_common(10)]\n",
    "# ax1.pie(fz_data, labels=fz_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"Generally MCWs*\", color = 'Blue')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(0,1))\n",
    "# labels=[f'{x} {np.round(y/sum(ps_data)*100,1)}%' for x,y in ps_count.most_common(10)]\n",
    "# ax1.pie(ps_data, labels=ps_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Positive Posts\", color = 'Green')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,0))\n",
    "# labels=[f'{x} {np.round(y/sum(ng_data)*100,1)}%' for x,y in ng_count.most_common(10)]\n",
    "# ax1.pie(ng_data, labels=ng_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Negative Posts\", color = 'Red')\n",
    "\n",
    "# ax1 = plt.subplot2grid((2,2),(1,1))\n",
    "# labels=[f'{x} {np.round(y/sum(nt_data)*100,1)}%' for x,y in nt_count.most_common(10)]\n",
    "# ax1.pie(nt_data, labels=nt_labels, startangle=140, rotatelabels=True)  # No %\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# fig1 = plt.gcf()\n",
    "# fig1.set_size_inches(5,5)\n",
    "# circle = plt.Circle(xy=(0,0), radius=0.7, facecolor='white')\n",
    "# plt.gca().add_artist(circle)\n",
    "# plt.title(\"MCWs* in Neutral Posts\", color = 'Grey')\n",
    "\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# Plot\n",
    "# plt.pie(data, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "# plt.axis('equal')\n",
    "plt.show()\n",
    "print(\"\\033[1;33m*MCW = Most Common Words\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = Counter(fusedTokens)\n",
    "# print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(positives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(negatives)\n",
    "# print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "# count = Counter(neutrals)\n",
    "# print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"black\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"red\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(0,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"green\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,0))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"blue\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.subplot2grid((2,2),(1,1))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "#     print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=7000)\n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets  vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bigbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=7000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"bigtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize = (16,16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy = (x[i], y[i]), xytext = (5,2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum  = 1\n",
    "allDicts = []\n",
    "\n",
    "dictLocation = \"./lexica/generic/generic.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "genericDict = {}\n",
    "for line in file:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(float(word))\n",
    "        count += 1\n",
    "    genericDict[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([genericDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictNum += 1\n",
    "\n",
    "# dictLocation = \"./lexica/emotweet/valence_tweet.txt\"\n",
    "# file = open(dictLocation, \"r\")\n",
    "# vl_dic = []\n",
    "# for line in file:\n",
    "#     print(line)\n",
    "#     temp = []\n",
    "#     count = 1\n",
    "#     for word in line.split():\n",
    "#         print(word)\n",
    "#         if count == 1:\n",
    "#             temp.append(word)\n",
    "#         else:\n",
    "#             temp.append(float(word))\n",
    "#         count += 1\n",
    "#     vl_dic.extend([temp])\n",
    "    \n",
    "# allDicts.extend([vl_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/affin/affin.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "af_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "\n",
    "#     af_dic.extend([temp])\n",
    "    af_dic[temp[0]] = temp[1]\n",
    "    \n",
    "allDicts.extend([af_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrc/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrc_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrc_dic[temp[0]] = temp[1]\n",
    "#     nrc_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrc_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum += 1\n",
    "\n",
    "dictLocation = \"./lexica/nrctag/val.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "nrctag_dic = {}\n",
    "\n",
    "for line in file:\n",
    "    temp = []\n",
    "    split_phrase = []\n",
    "\n",
    "    for word in line.split():\n",
    "        split_phrase.append(word)\n",
    "    \n",
    "    tmp_str = \"\"\n",
    "    for ph_word in split_phrase[:-1]:\n",
    "        if tmp_str == \"\":\n",
    "            tmp_str += ph_word\n",
    "        else:\n",
    "            tmp_str += \" \" + ph_word\n",
    "    \n",
    "    temp.append(tmp_str)\n",
    "    temp.append(float(split_phrase[-1]))\n",
    "    nrctag_dic[temp[0]] = temp[1]\n",
    "#     nrctag_dic.extend([temp])\n",
    "    \n",
    "allDicts.extend([nrctag_dic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the tweet vectors and adding the dictionary values to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"bigwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"bigwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow_xtrain, emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcBow = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain_bow: bag of words features for train data\n",
    "# ytrain: train data labels\n",
    "svcBow = svcBow.fit(xtrain_bow, ytrain)\n",
    "\n",
    "# probPrediction = svcBow.predict_proba(xvalid_bow) #predict on the validation set\n",
    "prediction_int = svcBow.predict(xvalid_bow)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_bow_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predbow.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(tfidf , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcIdf = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: tfidf features for train data\n",
    "# ytrain: train data labels\n",
    "svcIdf = svcIdf.fit(xtrain, ytrain) \n",
    "\n",
    "# probPrediction = svcIdf.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_tdidf_score = f1_score(yvalid, prediction_int, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetFeatsList , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V = svcW2V.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with W2V while not using dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetNod , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcW2V_Nod = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "\n",
    "# xtrain: word2vec features for train data\n",
    "# ytrain: train data labels\n",
    "svcW2V_Nod = svcW2V_Nod.fit(xtrain, ytrain)\n",
    "\n",
    "# probPrediction = svcW2V.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "# evaluate on the validation set\n",
    "tmp_svm_w2v_score_nod = f1_score(yvalid, prediction_int , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(tmp_svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same type of predictions with the test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/test2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)\n",
    "emoLocation = './twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "emoFile = open(emoLocation, \"r\")\n",
    "testEmotions = []\n",
    "for line in emoFile:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 2:\n",
    "            temp.append(word)\n",
    "        count += 1\n",
    "    testEmotions.extend(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "# dl = ndf.values.tolist()\n",
    "\n",
    "dl = df.values.tolist()\n",
    "\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "    tweet = item[3]\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "#     print(\"TEMP 3 = \",temp)\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    newTokens.append(final)\n",
    "    \n",
    "bow_xtest = bow_vectorizer.transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets  vocabulary_size) \n",
    "print(bow_xtest.shape)\n",
    "\n",
    "outfile = open(\"testbow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtest , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "outfile = open(\"testtfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresSize = 300\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = featuresSize, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the vectors and adding the dictionary parameteres to them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "allTweetNoDict = []\n",
    "\n",
    "for sentence in tokens:\n",
    "#     print(\"\\n\",sentence)\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    tweetNod = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "            tweetNod.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "            tweetNod.append(0)\n",
    "    \n",
    "    allTweetNoDict.extend([tweetNod])\n",
    "    \n",
    "    for dic in allDicts:\n",
    "#         print(dic)\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in sentence:\n",
    "#             print(word)\n",
    "            if word in dic:\n",
    "                wordCount += 1\n",
    "                value     += dic[word]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "# #         print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"testwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open(\"testwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetNoDict , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data BoW :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcBow.predict_proba(bow_xtrain) #predict on the validation set\n",
    "prediction_int = svcBow.predict(bow_xtest)\n",
    "prediction_int = prediction_int.tolist()\n",
    "svm_bow_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(svm_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcIdf.predict_proba(tfidf) #predict on the validation set\n",
    "prediction_int = svcIdf.predict(tfidf)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_tdidf_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(svm_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V.predict(allTweetFeatsList)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for test data W2V without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "allTweetNoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probPrediction = svcW2V.predict_proba(allTweetFeatsList) #predict on the validation set\n",
    "prediction_int = svcW2V_Nod.predict(allTweetNoDict)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "svm_w2v_score_nod = f1_score(testEmotions, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(svm_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the Bow data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigbow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(bow_xtrain , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testbow.pkl\" , \"rb\")\n",
    "bow_xtest = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(bow_xtest)\n",
    "knn_bow_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestbow.pkl\" , \"wb\")\n",
    "pickle.dump(knn_bow_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the TFIDF data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigtfidf.pkl\" , \"rb\")\n",
    "tfidf = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(tfidf , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testtfidf.pkl\" , \"rb\")\n",
    "tfidf_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(tfidf_new)\n",
    "knn_tdidf_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtesttfidf.pkl\" , \"wb\")\n",
    "pickle.dump(knn_tdidf_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up KNN and predicting by using the W2V data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbs.pkl\" , \"rb\")\n",
    "w2v = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbs.pkl\" , \"rb\")\n",
    "w2v_new = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new)\n",
    "knn_w2v_score = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction by using W2V data without the use of dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bigwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_NoDict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(w2v_NoDict , emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"testwordEmbsNoDict.pkl\" , \"rb\")\n",
    "w2v_new_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(w2v_new_nod)\n",
    "knn_w2v_score_nod = f1_score(testEmotions, prediction_knn , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"wb\")\n",
    "pickle.dump(knn_w2v_score_nod , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays with the success percentages of the algorithms used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"predtestbow.pkl\" , \"rb\")\n",
    "svm_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtesttfidf.pkl\" , \"rb\")\n",
    "svm_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbs.pkl\" , \"rb\")\n",
    "svm_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "svm_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_bow_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_tfidf_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"knn_predtestwordEmbsNoDict.pkl\" , \"rb\")\n",
    "knn_w2v_score_nod = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "col_labels = ['Bow', 'Tf_idf', 'Word_Emb', 'Word_Emb\\nwith dictionaries']\n",
    "row_labels = ['SVM', 'KNN']\n",
    "table_vals = [[str(float('%.2f'%(svm_bow_score * 100))) + \"%\", str(float('%.2f'%(svm_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(svm_w2v_score * 100))) + \"%\", str(float('%.2f'%(svm_w2v_score_nod * 100))) + \"%\"], \n",
    "              [str(float('%.2f'%(knn_bow_score * 100))) + \"%\", str(float('%.2f'%(knn_tfidf_score   * 100))) + \"%\", \n",
    "               str(float('%.2f'%(knn_w2v_score * 100))) + \"%\", str(float('%.2f'%(knn_w2v_score_nod * 100))) + \"%\"]]\n",
    "\n",
    "# Draw table\n",
    "the_table = plt.table(cellText  = table_vals,\n",
    "                      colWidths = [0.1, 0.1, 0.12, 0.18],\n",
    "                      rowLabels = row_labels,\n",
    "                      colLabels = col_labels,\n",
    "                      loc       = 'center'  ,\n",
    "                      cellLoc   = 'center',\n",
    "                      rowColours=('#FFFF00','#569857'),\n",
    "                      colColours=(\"#321789\",\"#321789\",\"#321789\",\"#321789\"))\n",
    "\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(24)\n",
    "the_table.scale(4, 4)\n",
    "\n",
    "cellDict = the_table.get_celld()\n",
    "for i in range(0,len(col_labels)):\n",
    "    cellDict[(0,i)].set_height(.3)\n",
    "\n",
    "# Removing ticks and spines enables you to get the figure only with table\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "for pos in ['right','top','bottom','left']:\n",
    "    plt.gca().spines[pos].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BOW   TD_IDF WORD_EMB\n",
    "SVM 51%     55%    42%\n",
    "KNN 48%     48%    48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              charts. \n",
    "       .\n",
    "     word embeddings  dictionaries      \n",
    "     '    .\n",
    "\n",
    "Edit:     ,      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
